{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e18fafc9-4031-4400-b598-242a8a2d1e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import re\n",
    "import io\n",
    "import requests\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fedb5fef-e4a6-4e5d-93d9-5b4395f74fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Retail Store Sales dataset \n",
    "#step 1\n",
    "url = \"Building_Permits.csv\"\n",
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e560222a-5709-4193-a2fd-f1fd870762a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Permit Number                                 0\n",
       "Permit Type                                   0\n",
       "Permit Type Definition                        0\n",
       "Permit Creation Date                          0\n",
       "Block                                         0\n",
       "Lot                                           0\n",
       "Street Number                                 0\n",
       "Street Number Suffix                      23366\n",
       "Street Name                                   0\n",
       "Street Suffix                               338\n",
       "Unit                                      20177\n",
       "Unit Suffix                               23406\n",
       "Description                                   9\n",
       "Current Status                                0\n",
       "Current Status Date                           0\n",
       "Filed Date                                    0\n",
       "Issued Date                                   0\n",
       "Completed Date                            14327\n",
       "First Construction Document Date          23687\n",
       "Structural Notification                   23080\n",
       "Number of Existing Stories                 1933\n",
       "Number of Proposed Stories                 1988\n",
       "Voluntary Soft-Story Retrofit             23789\n",
       "Fire Only Permit                          20319\n",
       "Permit Expiration Date                     1457\n",
       "Estimated Cost                              914\n",
       "Revised Cost                                  0\n",
       "Existing Use                               2624\n",
       "Existing Units                             3868\n",
       "Proposed Use                               2885\n",
       "Proposed Units                             3727\n",
       "Plansets                                    763\n",
       "TIDF Compliance                           23789\n",
       "Existing Construction Type                 2091\n",
       "Existing Construction Type Description     2091\n",
       "Proposed Construction Type                 2154\n",
       "Proposed Construction Type Description     2154\n",
       "Site Permit                               23347\n",
       "Supervisor District                         178\n",
       "Neighborhoods - Analysis Boundaries         178\n",
       "Zipcode                                      77\n",
       "Location                                    178\n",
       "Record ID                                     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da682523-ca74-4572-858f-f8c579e39d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_colname(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize column names:\n",
    "      - lower case\n",
    "      - replace spaces and punctuation with underscores\n",
    "      - remove repeated underscores\n",
    "    \"\"\"\n",
    "    name = name.lower()\n",
    "    name = re.sub(r\"[^\\w]+\", \"_\", name)      # non-word -> underscore\n",
    "    name = re.sub(r\"_+\", \"_\", name)          # collapse multiple underscores\n",
    "    name = name.strip(\"_\")\n",
    "    return name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28ecceb5-9ac2-4fce-a9c0-b47f6b27de02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numeric_safely(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Convert a pandas Series to numeric by stripping commas/currency symbols and coercing errors to NaN.\n",
    "    Useful for cost/value columns that may contain \"$\", \",\", or text like \"N/A\".\n",
    "    \"\"\"\n",
    "    return pd.to_numeric(series.astype(str).str.replace(r\"[^\\d\\.\\-]\", \"\", regex=True), errors=\"coerce\")\n",
    "\n",
    "def mode_or_nan(series: pd.Series):\n",
    "    \"\"\"Return mode if exists else NaN\"\"\"\n",
    "    try:\n",
    "        m = series.mode(dropna=True)\n",
    "        return m.iloc[0] if not m.empty else np.nan\n",
    "    except Exception:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f85f7f8b-615c-44f1-8b6a-17bc284ffe72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw rows downloaded: 23789\n"
     ]
    }
   ],
   "source": [
    "print(f\"Raw rows downloaded: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98abd83a-1872-402e-9085-423f0809463f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns after normalization: ['permit_number', 'permit_type', 'permit_type_definition', 'permit_creation_date', 'block', 'lot', 'street_number', 'street_number_suffix', 'street_name', 'street_suffix', 'unit', 'unit_suffix', 'description', 'current_status', 'current_status_date', 'filed_date', 'issued_date', 'completed_date', 'first_construction_document_date', 'structural_notification', 'number_of_existing_stories', 'number_of_proposed_stories', 'voluntary_soft_story_retrofit', 'fire_only_permit', 'permit_expiration_date', 'estimated_cost', 'revised_cost', 'existing_use', 'existing_units', 'proposed_use']\n"
     ]
    }
   ],
   "source": [
    "# --- Step 2: Normalize column names -------------------------------------\n",
    "df.columns = [clean_colname(c) for c in df.columns]\n",
    "print(\"Columns after normalization:\", list(df.columns)[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad6c0dd4-16d8-4d68-8edc-0a79970b2727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3: Quick inspection & missingness report -----------------------\n",
    "# Compute missingness percentages\n",
    "missing_pct = df.isnull().mean().sort_values(ascending=False) * 100\n",
    "missing_summary = pd.DataFrame({\n",
    "    \"column\": missing_pct.index,\n",
    "    \"missing_percent\": missing_pct.values,\n",
    "    \"non_null_count\": df.shape[0] - df.isnull().sum().values\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "082ec5fc-3f1e-4d2c-8727-319a895da0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 15 columns by missing %:\n",
      "                                     column  missing_percent  non_null_count\n",
      "0                          tidf_compliance       100.000000           23789\n",
      "1            voluntary_soft_story_retrofit       100.000000           23789\n",
      "2         first_construction_document_date        99.571230           23789\n",
      "3                              unit_suffix        98.390012           23789\n",
      "4                     street_number_suffix        98.221867           23789\n",
      "5                              site_permit        98.141998           23789\n",
      "6                  structural_notification        97.019631           23789\n",
      "7                         fire_only_permit        85.413426             423\n",
      "8                                     unit        84.816512           23789\n",
      "9                           completed_date        60.225314           23451\n",
      "10                          existing_units        16.259616            3612\n",
      "11                          proposed_units        15.666905             383\n",
      "12                            proposed_use        12.127454           23780\n",
      "13                            existing_use        11.030308           23789\n",
      "14  proposed_construction_type_description         9.054605           23789\n"
     ]
    }
   ],
   "source": [
    "# Print top 15 most-missing columns for quick view\n",
    "print(\"Top 15 columns by missing %:\\n\", missing_summary.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f7bed44-45f5-4192-86cc-d3d58641a6df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date-like columns detected: ['permit_creation_date', 'current_status_date', 'filed_date', 'issued_date', 'completed_date', 'first_construction_document_date', 'permit_expiration_date']\n"
     ]
    }
   ],
   "source": [
    "# --- Step 4: Parse common date columns (if present) -----------------------\n",
    "# Common date-like columns in permits data: 'application_date', 'issue_date', 'complete_date', 'filing_date', etc.\n",
    "date_cols_candidates = [c for c in df.columns if any(k in c for k in (\"date\", \"time\", \"issued\", \"appl\", \"filed\"))]\n",
    "print(\"Date-like columns detected:\", date_cols_candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a5739b7f-a681-468e-bef5-1b32c4c44bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_6060\\4284074536.py:5: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df[c] = pd.to_datetime(df[c], errors=\"coerce\", infer_datetime_format=True)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_6060\\4284074536.py:5: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df[c] = pd.to_datetime(df[c], errors=\"coerce\", infer_datetime_format=True)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_6060\\4284074536.py:5: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df[c] = pd.to_datetime(df[c], errors=\"coerce\", infer_datetime_format=True)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_6060\\4284074536.py:5: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df[c] = pd.to_datetime(df[c], errors=\"coerce\", infer_datetime_format=True)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_6060\\4284074536.py:5: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df[c] = pd.to_datetime(df[c], errors=\"coerce\", infer_datetime_format=True)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_6060\\4284074536.py:5: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df[c] = pd.to_datetime(df[c], errors=\"coerce\", infer_datetime_format=True)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_6060\\4284074536.py:5: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df[c] = pd.to_datetime(df[c], errors=\"coerce\", infer_datetime_format=True)\n"
     ]
    }
   ],
   "source": [
    "def try_parse_dates(df, cols):\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            # try a few common formats, coerce errors to NaT\n",
    "            df[c] = pd.to_datetime(df[c], errors=\"coerce\", infer_datetime_format=True)\n",
    "    return df\n",
    "\n",
    "df = try_parse_dates(df, date_cols_candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f195eac8-e643-4236-ae4d-b7bc14adbfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create derived time features for the most important columns, if present\n",
    "for col in (\"application_date\", \"issue_date\", \"complete_date\", \"filing_date\"):\n",
    "    if col in df.columns:\n",
    "        df[f\"{col}_year\"] = df[col].dt.year\n",
    "        df[f\"{col}_month\"] = df[col].dt.month\n",
    "        df[f\"{col}_dayofweek\"] = df[col].dt.dayofweek\n",
    "\n",
    "# Example: compute time-to-issue if both dates exist\n",
    "if \"application_date\" in df.columns and \"issue_date\" in df.columns:\n",
    "    df[\"days_to_issue\"] = (df[\"issue_date\"] - df[\"application_date\"]).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b55a693b-5fa4-4b4e-afe9-05348b625b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location-ish columns found: ['location']\n"
     ]
    }
   ],
   "source": [
    "# --- Step 5: Handle location / geometry column ----------------------------\n",
    "# Many DataSF datasets have a 'location' or 'geocoded_column' nested JSON; sometimes latitude/longitude provided.\n",
    "# We'll attempt to extract common fields: latitude, longitude, location_address\n",
    "loc_cols = [c for c in df.columns if \"location\" in c or \"latitude\" in c or \"longitude\" in c]\n",
    "print(\"Location-ish columns found:\", loc_cols)\n",
    "\n",
    "# If there is a single 'location' column containing JSON with latitude/longitude, parse it.\n",
    "if \"location\" in df.columns:\n",
    "    # some rows might have JSON-like strings; try to parse common patterns\n",
    "    def extract_lat(row):\n",
    "        try:\n",
    "            v = row\n",
    "            if pd.isna(v): return np.nan\n",
    "            # if looks like {\"latitude\":..., \"longitude\":...}\n",
    "            if isinstance(v, str) and (\"latitude\" in v or \"lon\" in v or \"coordinates\" in v):\n",
    "                # crude extraction of floating numbers\n",
    "                nums = re.findall(r\"-?\\d+\\.\\d+\", v)\n",
    "                if len(nums) >= 2:\n",
    "                    # assume lat, lon or lon,lat depending on format; choose heuristic: lat ~ between -90 and 90\n",
    "                    a, b = float(nums[0]), float(nums[1])\n",
    "                    if -90 <= a <= 90: return a\n",
    "                    if -90 <= b <= 90: return b\n",
    "            return np.nan\n",
    "        except Exception:\n",
    "            return np.nan\n",
    "    def extract_lon(row):\n",
    "        try:\n",
    "            v = row\n",
    "            if pd.isna(v): return np.nan\n",
    "            if isinstance(v, str) and (\"latitude\" in v or \"lon\" in v or \"coordinates\" in v):\n",
    "                nums = re.findall(r\"-?\\d+\\.\\d+\", v)\n",
    "                if len(nums) >= 2:\n",
    "                    a, b = float(nums[0]), float(nums[1])\n",
    "                    if -180 <= b <= 180: return b\n",
    "                    if -180 <= a <= 180: return a\n",
    "            return np.nan\n",
    "        except Exception:\n",
    "            return np.nan\n",
    "\n",
    "    df[\"latitude\"] = df[\"location\"].apply(extract_lat)\n",
    "    df[\"longitude\"] = df[\"location\"].apply(extract_lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "48677ac3-e065-4728-823c-55049aa46c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If lat/long exist in other columns, ensure numeric type\n",
    "for c in [\"latitude\", \"longitude\"]:\n",
    "    if c in df.columns:\n",
    "        df[c] = to_numeric_safely(df[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b098d463-ef89-423e-af43-133e15cb90c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible money columns: ['estimated_cost', 'revised_cost']\n"
     ]
    }
   ],
   "source": [
    "# --- Step 6: Normalize numeric / currency fields --------------------------\n",
    "# Frequently: 'estimated_cost', 'job_value', 'work_cost', or 'total_cost' appear as text with $ and commas\n",
    "possible_money_cols = [c for c in df.columns if any(k in c for k in (\"cost\",\"value\",\"fee\",\"amt\",\"estimate\",\"estimated\",\"valuation\",\"job_value\"))]\n",
    "print(\"Possible money columns:\", possible_money_cols)\n",
    "for c in possible_money_cols:\n",
    "    df[c] = to_numeric_safely(df[c])\n",
    "    # create flag for originally-missing\n",
    "    df[f\"{c}_missing_flag\"] = df[c].isna().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "40a184a1-3ab7-4a23-86e4-4ab7cf920655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical-like columns: ['permit_number', 'permit_type', 'permit_type_definition', 'permit_creation_date', 'description', 'current_status', 'current_status_date', 'fire_only_permit', 'permit_expiration_date', 'existing_use', 'proposed_use', 'existing_construction_type', 'existing_construction_type_description', 'proposed_construction_type', 'proposed_construction_type_description', 'site_permit']\n"
     ]
    }
   ],
   "source": [
    "# --- Step 7: Clean and standardize key categorical columns ----------------\n",
    "# Examples: 'permit_type', 'permit_status', 'permit_subtype', 'job_description', 'work_type', 'application_status'\n",
    "cat_candidates = [c for c in df.columns if any(k in c for k in (\"permit\",\"status\",\"type\",\"job\",\"work\",\"description\",\"use\"))]\n",
    "print(\"Categorical-like columns:\", cat_candidates)\n",
    "\n",
    "def clean_text_col(series: pd.Series) -> pd.Series:\n",
    "    # standard cleaning: strip, lower, remove extra whitespace, normalize common tokens\n",
    "    s = series.fillna(\"\").astype(str).str.strip()\n",
    "    s = s.str.replace(r\"\\s+\", \" \", regex=True)\n",
    "    s = s.str.replace(r\"[^0-9a-zA-Z\\s\\-\\_\\,\\&\\/\\(\\)]\", \"\", regex=True)  # allow some punctuation\n",
    "    s = s.str.lower()\n",
    "    s = s.replace({\"\": np.nan})\n",
    "    return s\n",
    "\n",
    "for c in cat_candidates:\n",
    "    df[c] = clean_text_col(df[c])\n",
    "# Example: collapse long text 'job_description' into a shorter 'job_short' (first 80 chars)\n",
    "if \"job_description\" in df.columns:\n",
    "    df[\"job_short\"] = df[\"job_description\"].astype(str).str.slice(0, 120).str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "96afe68b-a328-430c-8153-ee32aac99d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\nanfunctions.py:1215: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\nanfunctions.py:1215: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Step 8: Handle missing values intelligently --------------------------\n",
    "# Strategy used here:\n",
    "#  - For numeric columns: impute with median + keep a missing-flag column (already created for money columns)\n",
    "#  - For categorical columns: impute with mode and create missing-flag\n",
    "#  - For address components: keep as-is but try to extract clean 'street' and 'zipcode' if possible\n",
    "\n",
    "# Numeric imputation example\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "# exclude automatically created flags and lat/long if you prefer; here we impute only real numeric columns (not flags)\n",
    "numeric_to_impute = [c for c in numeric_cols if not c.endswith(\"_flag\") and c not in (\"latitude\",\"longitude\")]\n",
    "for c in numeric_to_impute:\n",
    "    med = df[c].median(skipna=True)\n",
    "    df[f\"{c}_missing_flag\"] = df[c].isna().astype(int)\n",
    "    df[c] = df[c].fillna(med)\n",
    "\n",
    "# Categorical imputation example (mode)\n",
    "categorical_cols = [c for c in df.columns if df[c].dtype == \"object\"]\n",
    "categorical_to_impute = [c for c in categorical_cols if df[c].isnull().mean() > 0 and df[c].nunique(dropna=True) < 500]\n",
    "for c in categorical_to_impute:\n",
    "    mode_val = mode_or_nan(df[c])\n",
    "    df[f\"{c}_missing_flag\"] = df[c].isna().astype(int)\n",
    "    if pd.notna(mode_val):\n",
    "        df[c] = df[c].fillna(mode_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7b58127a-ce4f-4e92-812d-e61987fefec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 9: Address parsing (simple heuristics) --------------------------\n",
    "# If there is an 'address' column, attempt to extract zip code and street number\n",
    "addr_col = None\n",
    "for candidate in (\"job_address\", \"address\", \"full_address\", \"street_address\"):\n",
    "    if candidate in df.columns:\n",
    "        addr_col = candidate\n",
    "        break\n",
    "\n",
    "if addr_col:\n",
    "    # extract zipcode: last 5-digit group in the string\n",
    "    df[\"zipcode\"] = df[addr_col].astype(str).str.extract(r\"(\\d{5})(?:-?\\d{0,4})?$\", expand=False)\n",
    "    # extract street number (first number group)\n",
    "    df[\"street_number\"] = df[addr_col].astype(str).str.extract(r\"^\\s*(\\d+)\\s+\", expand=False)\n",
    "    # street name (very naive: remove leading number and trailing zip)\n",
    "    df[\"street_name\"] = df[addr_col].astype(str).str.replace(r\"^\\s*\\d+\\s+\", \"\", regex=True).str.replace(r\"\\s+\\d{5}$\", \"\", regex=True).str.strip()\n",
    "    # clean blanks to NaN\n",
    "    df[\"zipcode\"] = df[\"zipcode\"].replace({\"\": np.nan})\n",
    "    df[\"street_number\"] = df[\"street_number\"].replace({\"\": np.nan})\n",
    "    df[\"street_name\"] = df[\"street_name\"].replace({\"\": np.nan})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "077bf79f-83e3-47ab-851b-203eae031a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 0 duplicate rows.\n",
      "Dropping extremely sparse columns: ['tidf_compliance', 'voluntary_soft_story_retrofit', 'first_construction_document_date', 'unit_suffix', 'street_number_suffix', 'site_permit', 'structural_notification']\n"
     ]
    }
   ],
   "source": [
    "# --- Step 10: Remove duplicates & low-information columns -----------------\n",
    "pre_dupe = len(df)\n",
    "df = df.drop_duplicates()\n",
    "post_dupe = len(df)\n",
    "print(f\"Removed {pre_dupe - post_dupe} duplicate rows.\")\n",
    "\n",
    "# Drop columns with > 95% missing (low information)\n",
    "high_missing_cols = missing_summary[missing_summary[\"missing_percent\"] > 95][\"column\"].tolist()\n",
    "# only drop if they still exist in df\n",
    "cols_to_drop = [c for c in high_missing_cols if c in df.columns]\n",
    "print(\"Dropping extremely sparse columns:\", cols_to_drop)\n",
    "df = df.drop(columns=cols_to_drop, errors=\"ignore\")\n",
    "\n",
    "# --- Step 11: Cast final datatypes for compactness ------------------------\n",
    "# Convert small-int-year columns to Int64 (nullable integer) where appropriate\n",
    "for c in df.columns:\n",
    "    if re.search(r\"_year$|_month$|_dayofweek$|_flag$\", c) and c in df.columns:\n",
    "        try:\n",
    "            df[c] = df[c].astype(\"Int64\")\n",
    "        except Exception:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b1c91b67-1805-4455-988b-47e0a69a9c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final summary: {'rows_final': 23789, 'columns_final': 65, 'sample_columns': ['permit_number', 'permit_type', 'permit_type_definition', 'permit_creation_date', 'block', 'lot', 'street_number', 'street_name', 'street_suffix', 'unit', 'description', 'current_status', 'current_status_date', 'filed_date', 'issued_date', 'completed_date', 'number_of_existing_stories', 'number_of_proposed_stories', 'fire_only_permit', 'permit_expiration_date', 'estimated_cost', 'revised_cost', 'existing_use', 'existing_units', 'proposed_use', 'proposed_units', 'plansets', 'existing_construction_type', 'existing_construction_type_description', 'proposed_construction_type', 'proposed_construction_type_description', 'supervisor_district', 'neighborhoods_analysis_boundaries', 'zipcode', 'location', 'record_id', 'estimated_cost_missing_flag', 'revised_cost_missing_flag', 'latitude', 'longitude']}\n"
     ]
    }
   ],
   "source": [
    "# --- Step 12: Final summary & save --------------------------------------\n",
    "summary = {\n",
    "    \"rows_final\": len(df),\n",
    "    \"columns_final\": df.shape[1],\n",
    "    \"sample_columns\": list(df.columns)[:40]\n",
    "}\n",
    "print(\"Final summary:\", summary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "41f5d2ad-90a0-4f8d-a4f6-3bfedcd10b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset saved to output_clean_csv.csv\n",
      "Missingness report was saved to sf_missing_report.csv\n"
     ]
    }
   ],
   "source": [
    "# Save cleaned dataframe\n",
    "OUTPUT_CLEAN_CSV='output_clean_csv.csv'\n",
    "MISSING_REPORT = \"sf_missing_report.csv\"\n",
    "df.to_csv(OUTPUT_CLEAN_CSV, index=False)\n",
    "print(f\"Cleaned dataset saved to {OUTPUT_CLEAN_CSV}\")\n",
    "print(\"Missingness report was saved to\", MISSING_REPORT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2eef873b-0681-43a1-9060-e3b359e601e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "permit_number                                          0\n",
       "permit_type                                            0\n",
       "permit_type_definition                                 0\n",
       "permit_creation_date                                   0\n",
       "block                                                  0\n",
       "                                                      ..\n",
       "existing_construction_type_description_missing_flag    0\n",
       "proposed_construction_type_missing_flag                0\n",
       "proposed_construction_type_description_missing_flag    0\n",
       "site_permit_missing_flag                               0\n",
       "neighborhoods_analysis_boundaries_missing_flag         0\n",
       "Length: 65, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
